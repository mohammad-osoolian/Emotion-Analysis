# -*- coding: utf-8 -*-
"""Train_parsbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AuF1_kQFavK1tOvwuKqmI6w3MOEioDDe
"""

! pip install -q accelerate datasets evaluate sacremoses

!pip install git+https://github.com/Dadmatech/DadmaTools.git

import torch
import torch.nn as nn
import numpy as np
from datasets import load_dataset
import evaluate
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, AutoModel
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorWithPadding, EvalPrediction
from transformers.optimization import AdamW
from time import time
from dadmatools.normalizer import Normalizer
from huggingface_hub import notebook_login
from transformers import set_seed


SEED = 42
set_seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)

notebook_login()

from google.colab import drive
drive.mount('/content/drive')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

normalizer = Normalizer(
    full_cleaning=False,
    unify_chars=True,
    refine_punc_spacing=True,
    remove_extra_space=True,
    remove_puncs=False,
    remove_html=True,
    remove_stop_word=False,
    replace_email_with="<EMAIL>",
    replace_number_with=None,
    replace_url_with="",
    replace_mobile_number_with=None,
    replace_emoji_with="",
    replace_home_number_with=None
)

classes = ['HAPPY', 'SAD', 'ANGRY', 'FEAR', 'SURPRISE', 'HATE', 'OTHER']
class2id = {classes[i]: i for i in range(len(classes))}
id2class = {i: classes[i] for i in range(len(classes))}

def sigmoid(X):
    return 1 / (1 + np.exp(-X))

def heaviside(X):
    return np.heaviside(X - 0.5, 0)

def onehot(ids, size=len(classes)):
  result = np.zeros((ids.shape[0], size))
  result[np.arange(ids.shape[0]), ids] = 1
  return result

def compute_metrics(eval_preds: EvalPrediction):
  logits, labels = eval_preds.predictions, eval_preds.label_ids
  predictions = onehot(np.argmax(sigmoid(logits), axis=-1))

  f1 = f1_score(labels, predictions, average=None, zero_division=0.0)
  f1 = {f'f1_C{i}': f1[i] for i in range(len(f1))}
  f1_macro = f1_score(labels, predictions, average='macro', zero_division=0.0)
  recall = recall_score(labels, predictions, average=None, zero_division=0.0)
  recall = {f'recall_C{i}': recall[i] for i in range(len(recall))}
  recall_macro = recall_score(labels, predictions, average='macro', zero_division=0.0)
  precision = precision_score(labels, predictions, average=None, zero_division=0.0)
  precision = {f'precision_C{i}': precision[i] for i in range(len(precision))}
  precision_macro = precision_score(labels, predictions, average='macro', zero_division=0.0)
  accuracy = accuracy_score(labels, predictions)
  results = {'accuracy': accuracy, 'precision_macro': precision_macro, 'recall_macro': recall_macro, 'f1_macro': f1_macro, **f1, **recall, **precision}
  return results

ds_url = f'/content/'
ds_files = {
    'train': ds_url + 'pptrain.tsv',
    'test': ds_url + 'pptest.tsv',
}

ds = load_dataset('csv', data_files=ds_files, delimiter='\t')
ds = ds.rename_columns({'ID': 'id', 'Text': 'text', 'Label': 'label'})

ds

def convert_labels(example):
  example["label"] = [float(num) for num in example['label'][1:-1].split(' ')]
  return example

def replace_none_with_str(example):
  if example['text'] == None:
    example['text'] = ''
  return example

def remove_hashtag(example):
  while '#' in example['text']:
    example['text'] = example['text'].replace('#', '')
  return example

ds = ds.map(convert_labels)
ds = ds.map(replace_none_with_str)
ds = ds.map(remove_hashtag)
print(ds['train'].features)
print(ds['test'].features)

num_epochs = 5
checkpoint = 'HooshvareLab/bert-base-parsbert-uncased'

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
def tokenize_function(example):
  example['text'] = [normalizer.normalize(t) for t in example['text']]
  return tokenizer(example['text'], truncation=True, max_length=512, add_special_tokens=True)

tokenized_datasets = ds.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer)

tokenized_datasets['train'].features

config = AutoConfig.from_pretrained(checkpoint)
config.update({
    'id2label': id2class,
    'label2id': class2id,
    'num_labels': 7
})
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=7)
model.config = config

model.config

training_args = TrainingArguments(
    run_name=f'First Run-{time()}',
    output_dir='finetuned-parsbert-uncased-ArmanEmo',
    overwrite_output_dir=True,
    auto_find_batch_size=True,
    num_train_epochs=num_epochs,
    evaluation_strategy='epoch',
    eval_steps=512,
    save_strategy='epoch',
    save_steps=512,
    save_total_limit=5,
    load_best_model_at_end=True,
    metric_for_best_model='f1_macro',
    save_safetensors=True,
    group_by_length=True,
    push_to_hub=True,
    hub_model_id='mohammadhabp/finetuned-parsbert-uncased-ArmanEmo',
    hub_strategy='all_checkpoints',
    hub_token='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',
    warmup_steps=500,
    weight_decay=4e-3,
    seed=SEED,
    data_seed=SEED
)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

train_output = trainer.train()

trainer.save_model()
trainer.push_to_hub('fine tuned model')