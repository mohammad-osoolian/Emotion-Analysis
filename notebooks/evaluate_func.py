# -*- coding: utf-8 -*-
"""Evaluate_func.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gGbEWC4e8GZyctijm1Sds9X6ZXm0axoC
"""

# from huggingface_hub import notebook_login

# notebook_login()

from transformers import pipeline
classifier = pipeline("sentiment-analysis", model = "mohammad-osoolian/parsbert-finetuned")
# classifier = pipeline("sentiment-analysis", model = "mohammadhabp/finetuned-parsbert-uncased-ArmanEmo")

classifier.predict('حس خیلی خوبی دارم')

classifier.predict('امروز حال من خوب نیست')

classifier.predict('نگرانم برایش اتفاقی افتاده باشد')

"""# Evaluation"""

from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
import numpy as np

classes = ['HAPPY', 'SAD', 'ANGRY', 'FEAR', 'SURPRISE', 'HATE', 'OTHER']
class2id = {classes[i]: i for i in range(len(classes))}
id2class = {i: classes[i] for i in range(len(classes))}

def sigmoid(X):
    return 1 / (1 + np.exp(-X))

def heaviside(X):
    return np.heaviside(X - 0.5, 0)

def onehot(ids, size=len(classes)):
  result = np.zeros((len(ids), size))
  result[np.arange(len(ids)), ids] = 1
  return result

def compute_metrics(preds, labels):
  """
      preds: one_hot of predictions
      labels: one_hot of labels
  """

  predictions = preds
  labels = labels

  f1 = f1_score(labels, predictions, average=None, zero_division=0.0)
  f1 = {f'f1_C{i}': f1[i] for i in range(len(f1))}
  f1_macro = f1_score(labels, predictions, average='macro', zero_division=0.0)
  recall = recall_score(labels, predictions, average=None, zero_division=0.0)
  recall = {f'recall_C{i}': recall[i] for i in range(len(recall))}
  recall_macro = recall_score(labels, predictions, average='macro', zero_division=0.0)
  precision = precision_score(labels, predictions, average=None, zero_division=0.0)
  precision = {f'precision_C{i}': precision[i] for i in range(len(precision))}
  precision_macro = precision_score(labels, predictions, average='macro', zero_division=0.0)
  accuracy = accuracy_score(labels, predictions)
  results = {'accuracy': accuracy, 'precision_macro': precision_macro, 'recall_macro': recall_macro, 'f1_macro': f1_macro, **f1, **recall, **precision}
  return results

def evaluate(inpath, delim, classifier):
  sentences = []
  labels = []
  with open(inpath, 'r', encoding="utf-8-sig") as f:
    for line in f.readlines():
      sentence, label = line.strip().split(delim)
      labels.append(class2id[label])
      sentences.append(sentence)

  predictions = classifier.predict(sentences)
  preds = [class2id[predictions[i]['label']] for i in range(len(predictions))]

  labels = onehot(np.array(labels))
  preds = onehot(np.array(preds))

  return compute_metrics(preds, labels)

evaluate('/content/Project_Test - testset.tsv', '\t', classifier)

import pandas as pd

df = pd.read_csv('/content/Project_Test - testset.tsv', sep='\t')
df.to_csv('test.csv')

evaluate('/content/Project_Test - testset.tsv', '\t', classifier)