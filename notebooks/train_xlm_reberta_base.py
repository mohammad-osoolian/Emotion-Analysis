# -*- coding: utf-8 -*-
"""train_xlm_reberta_base.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IlsvbVbRLpf52x0KqSjjP_yPJTD5pUG3
"""

! pip install -q accelerate datasets evaluate

import torch
import torch.nn as nn
import numpy as np
from datasets import load_dataset, DatasetDict
import evaluate
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score

from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorWithPadding, EvalPrediction
from transformers.optimization import AdamW

from time import time
from transformers import set_seed
set_seed(365)

classes = ['HAPPY', 'SAD', 'ANGRY', 'FEAR', 'SURPRISE', 'HATE', 'OTHER']
class2id = {classes[i]: i for i in range(len(classes))}
id2class = {i: classes[i] for i in range(len(classes))}

def sigmoid(X):
    return 1 / (1 + np.exp(-X))

def heaviside(X):
    return np.heaviside(X - 0.5, 0)

def onehot(ids, size=len(classes)):
  # print(ids)
  result = np.zeros((ids.shape[0], size))
  result[np.arange(ids.shape[0]), ids] = 1
  return result


def compute_metrics(eval_preds: EvalPrediction):
  logits, labels = eval_preds.predictions, eval_preds.label_ids
  predictions = onehot(np.argmax(sigmoid(logits), axis=-1))

  f1 = f1_score(labels, predictions, average=None, zero_division=0.0)
  f1 = {f'f1_C{i}': f1[i] for i in range(len(f1))}
  f1_macro = f1_score(labels, predictions, average='macro', zero_division=0.0)
  recall = recall_score(labels, predictions, average=None, zero_division=0.0)
  recall = {f'recall_C{i}': recall[i] for i in range(len(recall))}
  recall_macro = recall_score(labels, predictions, average='macro', zero_division=0.0)
  precision = precision_score(labels, predictions, average=None, zero_division=0.0)
  precision = {f'precision_C{i}': precision[i] for i in range(len(precision))}
  precision_macro = precision_score(labels, predictions, average='macro', zero_division=0.0)
  accuracy = accuracy_score(labels, predictions)
  results = {'accuracy': accuracy, 'precision_macro': precision_macro, 'recall_macro': recall_macro, 'f1_macro': f1_macro, **f1, **recall, **precision}
  return results
  # return {'exhgh': 0.1}

imdb_train = load_dataset("imdb", split='train').shuffle(seed=0).shard(num_shards=10, index=0)
imdb_test = load_dataset("imdb", split='test').shuffle(seed=0).shard(num_shards=50, index=0)
# print(imdb_test['label'])
imdb = DatasetDict({'train': imdb_train, 'test': imdb_test})
ds = imdb

ds_url = f'/content/'
ds_files = {
    'train': ds_url + 'pptrain.tsv',
    'test': ds_url + 'pptest.tsv',
}

ds = load_dataset('csv', data_files=ds_files, delimiter='\t')
ds = ds.rename_columns({'ID': 'id', 'Text': 'text', 'Label': 'label'})

ds

def convert_labels(example):
  # result = np.zeros((2))
  # result[example['label']] = 1
  # example['label'] = result
  example["label"] = [float(num) for num in example['label'][1:-1].split(' ')]
  return example

def replace_none_with_str(example):
  if example['text'] == None:
    example['text'] = ''
  return example

ds = ds.map(convert_labels)
ds = ds.map(replace_none_with_str)

print(ds['train'].features)
print(ds['test'].features)

num_epochs = 5
checkpoint = 'FacebookAI/xlm-roberta-base'

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
def tokenize_function(example):
  return tokenizer(example['text'], truncation=True, max_length=256, add_special_tokens=True)

tokenized_datasets = ds.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer)

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=7)

training_args = TrainingArguments(
    run_name=f'First Run-{time()}',
    output_dir='outputs-xml', overwrite_output_dir=True,
    # auto_find_batch_size=True,
    num_train_epochs=num_epochs,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=5, load_best_model_at_end=True,
    push_to_hub=True,
    hub_model_id='mohammad-osoolian/DL-xlm-roberta-base10',
    hub_strategy='every_save',
    hub_private_repo=False,
    hub_token=''
)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

torch.cuda.empty_cache()

train_output = trainer.train()

train_output = trainer.train()

